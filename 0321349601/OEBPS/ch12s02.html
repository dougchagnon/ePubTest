<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Testing for Performance</title><link rel="stylesheet" href="styles/convert.css" type="text/css"/></head><body><div class="section" title="Testing for Performance"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lev1sec2"/>Testing for Performance</h2></div></div></div>

<p><a id="iddle1193" class="indexterm"/><a id="iddle1744" class="indexterm"/><a id="iddle2162" class="indexterm"/><a id="iddle2417" class="indexterm"/><a id="iddle2418" class="indexterm"/><a id="iddle3538" class="indexterm"/><a id="iddle4309" class="indexterm"/><a id="iddle4695" class="indexterm"/><a id="iddle4696" class="indexterm"/><a id="iddle4699" class="indexterm"/><a id="iddle4701" class="indexterm"/><a id="iddle4912" class="indexterm"/><a id="iddle4913" class="indexterm"/><a id="iddle5021" class="indexterm"/><a id="iddle5022" class="indexterm"/>Performance tests are often extended versions of functionality tests. In fact, it is almost always worthwhile to include some basic functionality testing within performance tests to ensure that you are not testing the performance of broken code.</p>
<p>While there is definitely overlap between performance and functionality tests, they have different goals. Performance tests seek to measure end-to-end performance metrics for representative use cases. Picking a reasonable set of usage scenarios is not always easy; ideally, tests should reflect how the objects being tested are actually used in your application.</p>
<p>In some cases an appropriate test scenario is obvious. Bounded buffers are nearly always used in producer-consumer designs, so it is sensible to measure the throughput of producers feeding data to consumers. We can easily extend <code class="literal">PutTakeTest</code> to become a performance test for this scenario.</p>
<p>A common secondary goal of performance testing is to select sizings empirically for various bounds—numbers of threads, buffer capacities, and so on. While these values might turn out to be sensitive enough to platform characteristics (such as processor type or even processor stepping level, number of CPUs, or memory size) to require dynamic configuration, it is equally common that reasonable choices for these values work well across a wide range of systems.</p>
<div class="section" title="Extending PutTakeTest to Add Timing"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lev2sec7"/>Extending PutTakeTest to Add Timing</h3></div></div></div>

<p>The primary extension we have to make to <code class="literal">PutTakeTest</code> is to measure the time taken for a run. Rather than attempting to measure the time for a single operation, we get a more accurate measure by timing the entire run and dividing by the number of operations to get a per-operation time. We are already using a <code class="literal">CyclicBarrier</code> to start and stop the worker threads, so we can extend this by using a barrier action that measures the start and end time, as shown in <a class="link" href="ch12s02.html#ch12list11" title="Example 12.11. Barrier-based Timer.">Listing 12.11</a>.</p>
<p>We can modify the initialization of the barrier to use this barrier action by using the constructor for <code class="literal">CyclicBarrier</code> that accepts a barrier action:</p>
<div class="example"><a id="ch12list11"/><p class="title"><b>Example 12.11. Barrier-based Timer.</b></p><div class="example-contents">

<pre class="programlisting">this.timer = new BarrierTimer();
this.barrier = new CyclicBarrier(npairs * 2 + 1, timer);
public class BarrierTimer implements Runnable {
    private boolean started;
    private long startTime, endTime;

    public synchronized void run() {
        long t = System.nanoTime();
        if (!started) {
            started = true;
            startTime = t;
        } else
            endTime = t;
    }
    public synchronized void clear() {
        started = false;
    }
    public synchronized long getTime() {
        return endTime - startTime;
    }
}
</pre>
</div></div><br class="example-break"/>
<p><a id="iddle1246" class="indexterm"/><a id="iddle1247" class="indexterm"/><a id="iddle1264" class="indexterm"/><a id="iddle1265" class="indexterm"/><a id="iddle1270" class="indexterm"/><a id="iddle1271" class="indexterm"/><a id="iddle1712" class="indexterm"/><a id="iddle2080" class="indexterm"/><a id="iddle2205" class="indexterm"/><a id="iddle3471" class="indexterm"/><a id="iddle3687" class="indexterm"/><a id="iddle4086" class="indexterm"/><a id="iddle4310" class="indexterm"/><a id="iddle4311" class="indexterm"/><a id="iddle4870" class="indexterm"/><a id="iddle4871" class="indexterm"/>The modified <code class="literal">test</code> method using the barrier-based timer is shown in <a class="link" href="ch12s02.html#ch12list12" title="Example 12.12. Testing with a Barrier-based Timer.">Listing 12.12</a>.</p>
<p>We can learn several things from running <code class="literal">TimedPutTakeTest</code>. One is the throughput of the producer-consumer handoff operation for various combinations of parameters; another is how the bounded buffer scales with different numbers of threads; a third is how we might select the bound size. Answering these questions requires running the test for various combinations of parameters, so we’ll need amain test driver, shown in <a class="link" href="ch12s02.html#ch12list13" title="Example 12.13. Driver Programfor TimedPutTakeTest.">Listing 12.13</a>.</p>
<p><a class="link" href="ch12s02.html#ch12fig01" title="Figure 12.1. TimedPutTakeTest with Various Buffer Capacities.">Figure 12.1</a> shows some sample results on a 4-way machine, using buffer capacities of 1, 10, 100, and 1000. We see immediately that a buffer size of one causes very poor throughput; this is because each thread can make only a tiny bit of progress before blocking and waiting for another thread. Increasing buffer size to ten helps dramatically, but increases past ten offer diminishing returns.</p>
<div class="figure-float"><div class="figure"><a id="ch12fig01"/><p class="title"><b>Figure 12.1. <code class="literal">TimedPutTakeTest</code> with Various Buffer Capacities.</b></p><div class="figure-contents">


<div class="mediaobject"><img src="graphics/12fig01.gif" height="302" alt="TimedPutTakeTest with Various Buffer Capacities."/></div>
</div></div><br class="figure-break"/></div>
<p>It may be somewhat puzzling at first that adding a lot more threads degrades performance only slightly. The reason is hard to see from the data, but easy to see on a CPU performance meter such as <code class="literal">perfbar</code> while the test is running: even with many threads, not much computation is going on, and most of it is spent blocking and unblocking threads. So there is plenty of CPU slack for more threads to do the same thing without hurting performance very much.</p>
<p>However, be careful about concluding from this data that you can always add more threads to a producer-consumer program that uses a bounded buffer. This test is fairly artificial in how it simulates the <span class="emphasis"><em>application</em></span>; the producers do almost no work to generate the item placed on the queue, and the consumers do almost no work with the item retrieved. If the worker threads in a real producer-consumer <a id="iddle1973" class="indexterm"/><a id="iddle1974" class="indexterm"/><a id="iddle4686" class="indexterm"/><a id="iddle1050" class="indexterm"/><a id="iddle1076" class="indexterm"/><a id="iddle1123" class="indexterm"/><a id="iddle1585" class="indexterm"/><a id="iddle1586" class="indexterm"/><a id="iddle2988" class="indexterm"/><a id="iddle4056" class="indexterm"/><a id="iddle4057" class="indexterm"/><a id="iddle4498" class="indexterm"/>application do some nontrivial work to produce and consume items (as is generally the case), then this slack would disappear and the effects of having too many threads could be very noticeable. The primary purpose of this test is to measure what constraints the producer-consumer handoff via the bounded buffer imposes on overall throughput.</p>
<div class="example"><a id="ch12list12"/><p class="title"><b>Example 12.12. Testing with a Barrier-based Timer.</b></p><div class="example-contents">


<pre class="programlisting">public void test() {
    try {
        timer.clear();
        for (int i = 0; i &lt; nPairs; i++) {
            pool.execute(new Producer());
            pool.execute(new Consumer());
        }
        barrier.await();
        barrier.await();
        long nsPerItem = timer.getTime() / (nPairs*  (long)nTrials);
        System.out.print("Throughput: " + nsPerItem + " ns/item");
        assertEquals(putSum.get(), takeSum.get());
    } catch (Exception e) {
        throw new RuntimeException(e);
    }
}
</pre>
</div></div><br class="example-break"/>
<div class="example"><a id="ch12list13"/><p class="title"><b>Example 12.13. Driver Programfor <code class="literal">TimedPutTakeTest</code>.</b></p><div class="example-contents">


<pre class="programlisting">public static void main(String[] args) throws Exception {
    int tpt = 100000;  // <span class="emphasis"><em>trials per thread</em></span>
    for (int cap = 1; cap &lt;= 1000; cap*= 10) {
        System.out.println("Capacity: " + cap);
        for (int pairs = 1; pairs &lt;= 128; pairs*= 2) {
            TimedPutTakeTest t =
                new TimedPutTakeTest(cap, pairs, tpt);
            System.out.print("Pairs: " + pairs + "\t");
            t.test();
            System.out.print("\t");
            Thread.sleep(1000);
            t.test();
            System.out.println();
            Thread.sleep(1000);
        }
    }
    pool.shutdown();
}
</pre>
</div></div><br class="example-break"/>
</div>
<div class="section" title="Comparing Multiple Algorithms"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lev2sec8"/>Comparing Multiple Algorithms</h3></div></div></div>

<p>While <code class="literal">BoundedBuffer</code> is a fairly solid implementation that performs reasonably well, it turns out to be no match for either <code class="literal">ArrayBlockingQueue</code> or <code class="literal">LinkedBlockingQueue</code> (which explains why this buffer algorithm wasn’t selected for inclusion in the class library). The <code class="literal">java.util.concurrent</code> algorithms have been selected and tuned, in part using tests just like those described here, to be as efficient as we know how to make them, while still offering a wide range of functionality.<sup>[<a id="ch12fn06" href="#ftn.ch12fn06" class="footnote">6</a>]</sup> The main reason <code class="literal">BoundedBuffer</code> fares poorly is that <code class="literal">put</code> and <code class="literal">take</code> each have multiple operations that could encouter contention—acquire a semaphore, acquire a lock, release a semaphore. Other implementation approaches have fewer points at which they might contend with another thread.</p>
<p><a class="link" href="ch12s02.html#ch12fig02" title="Figure 12.2. Comparing Blocking Queue Implementations.">Figure 12.2</a> shows comparative throughput on a dual hyperthreaded machine for all three classes with 256-element buffers, using a variant of <code class="literal">TimedPutTakeTest</code>. This test suggests that <code class="literal">LinkedBlockingQueue</code> scales better than <code class="literal">ArrayBlockingQueue</code>. This may seem odd at first: a linked queue must allocate a link node object for each insertion, and hence seems to be doing more work than the array-based queue. However, even though it has more allocation and <a id="iddle1402" class="indexterm"/><a id="iddle1403" class="indexterm"/><a id="iddle2473" class="indexterm"/><a id="iddle2474" class="indexterm"/><a id="iddle3188" class="indexterm"/><a id="iddle3649" class="indexterm"/><a id="iddle3744" class="indexterm"/><a id="iddle3745" class="indexterm"/><a id="iddle3957" class="indexterm"/><a id="iddle4614" class="indexterm"/><a id="iddle4901" class="indexterm"/><a id="iddle4902" class="indexterm"/><a id="iddle5069" class="indexterm"/><a id="iddle5070" class="indexterm"/>GC overhead, a linked queue allows more concurrent access by <code class="literal">put</code>s and <code class="literal">take</code>s than an array-based queue because the best linked queue algorithms allow the head and tail to be updated independently. Because allocation is usually threadlocal, algorithms that can reduce contention by doing more allocation usually scale better. (This is another instance in which intuition based on traditional performance tuning runs counter to what is needed for scalability.)</p>
<div class="figure-float"><div class="figure"><a id="ch12fig02"/><p class="title"><b>Figure 12.2. Comparing Blocking Queue Implementations.</b></p><div class="figure-contents">


<div class="mediaobject"><img src="graphics/12fig02.gif" height="255" alt="Comparing Blocking Queue Implementations."/></div>
</div></div><br class="figure-break"/></div>
</div>
<div class="section" title="Measuring Responsiveness"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lev2sec9"/>Measuring Responsiveness</h3></div></div></div>

<p>So far we have focused on measuring throughput, which is usually the most important performance metric for concurrent programs. But sometimes it is more important to know how long an individual action might take to complete, and in this case we want to measure the <span class="emphasis"><em>variance</em></span> of service time. Sometimes it makes sense to allow a longer average service time if it lets us obtain a smaller variance; predictability is a valuable performance characteristic too. Measuring variance allows us to estimate the answers to quality-of-service questions like “What percentage of operations will succeed in under 100 milliseconds?”</p>
<p>Histograms of task completion times are normally the best way to visualize variance in service time. Variances are only slightly more difficult to measure than averages—you need to keep track of per-task completion times in addition to aggregate completion time. Since timer granularity can be a factor in measuring individual task time (an individual task may take less than or close to the smallest “timer tick”, which would distort measurements of task duration), to avoid measurement artifacts we can measure the run time of small batches of <code class="literal">put</code> and <code class="literal">take</code> operations instead.</p>
<p><a id="iddle2324" class="indexterm"/><a id="iddle2325" class="indexterm"/><a id="iddle3308" class="indexterm"/><a id="iddle3309" class="indexterm"/><a id="iddle4153" class="indexterm"/><a id="iddle4154" class="indexterm"/><a id="iddle4155" class="indexterm"/><a id="iddle4156" class="indexterm"/><a class="link" href="ch12s02.html#ch12fig03" title="Figure 12.3. Completion Time Histogram for TimedPutTakeTest with Default (Nonfair) and Fair Semaphores.">Figure 12.3</a> shows the per-task completion times of a variant of <code class="literal">TimedPutTakeTest</code> using a buffer size of 1000 in which each of 256 concurrent tasks iterates only 1000 items for nonfair (shaded bars) and fair semaphores (open bars). (<a class="link" href="ch13s03.html" title="Fairness">Section 13.3</a> explains fair versus nonfair queueing for locks and semaphores.) Completion times for nonfair semaphores range from 104 to 8,714 ms, a factor of over eighty. It is possible to reduce this range by forcing more fairness in concurrency control; this is easy to do in <code class="literal">BoundedBuffer</code> by initializing the semaphores to fair mode. As <a class="link" href="ch12s02.html#ch12fig03" title="Figure 12.3. Completion Time Histogram for TimedPutTakeTest with Default (Nonfair) and Fair Semaphores.">Figure 12.3</a> shows, this succeeds in greatly reducing the variance (now ranging only from 38,194 to 38,207 ms), but unfortunately also greatly reduces the throughput. (A longer-running test with more typical kinds of tasks would probably show an even larger throughput reduction.)</p>
<div class="figure-float"><div class="figure"><a id="ch12fig03"/><p class="title"><b>Figure 12.3. Completion Time Histogram for <code class="literal">TimedPutTakeTest</code> with Default (Nonfair) and Fair Semaphores.</b></p><div class="figure-contents">


<div class="mediaobject"><img src="graphics/12fig03.gif" height="260" alt="Completion Time Histogram for TimedPutTakeTest with Default (Nonfair) and Fair Semaphores."/></div>
</div></div><br class="figure-break"/></div>
<p>We saw before that very small buffer sizes cause heavy context switching and poor throughput even in nonfair mode, because nearly every operation involves a context switch. As an indication that the cost of fairness results primarily from blocking threads, we can rerun this test with a buffer size of one and see that nonfair semaphores now perform comparably to fair semaphores. <a class="link" href="ch12s02.html#ch12fig04" title="Figure 12.4. Completion Time Histogram for TimedPutTakeTest with Single-item Buffers.">Figure 12.4</a> shows that fairness doesn’t make the average much worse or the variance much better in this case.</p>
<div class="figure-float"><div class="figure"><a id="ch12fig04"/><p class="title"><b>Figure 12.4. Completion Time Histogram for <code class="literal">TimedPutTakeTest</code> with Single-item Buffers.</b></p><div class="figure-contents">


<div class="mediaobject"><img src="graphics/12fig04.gif" height="304" alt="Completion Time Histogram for TimedPutTakeTest with Single-item Buffers."/></div>
</div></div><br class="figure-break"/></div>
<p>So, unless threads are continually blocking anyway because of tight synchronization requirements, nonfair semaphores provide much better throughput and fair semaphores provides lower variance. Because the results are so dramatically different, <code class="literal">Semaphore</code> forces its clients to decide which of the two factors to optimize for.</p>
</div>
<div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.ch12fn06" href="#ch12fn06" class="para">6</a>] </sup>You might be able to outperform them if you both are a concurrency expert and can give up some of the provided functionality.</p></div></div></div></body></html>

<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Amdahl’s Law</title><link rel="stylesheet" href="styles/convert.css" type="text/css"/></head><body><div class="section" title="Amdahl’s Law"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lev1sec2"/>Amdahl’s Law</h2></div></div></div>

<p>Some problems can be solved faster with more resources—the more workers available for harvesting crops, the faster the harvest can be completed. Other tasks are fundamentally serial—no number of additional workers will make the crops grow any faster. If one of our primary reasons for using threads is to harness the power of multiple processors, we must also ensure that the problem is amenable to parallel decomposition and that our program effectively exploits this potential for parallelization.</p>
<p>Most concurrent programs have a lot in common with farming, consisting of a mix of parallelizable and serial portions. <span class="emphasis"><em>Amdahl’s law</em></span> describes how much a program can theoretically be sped up by additional computing resources, based on the proportion of parallelizable and serial components. If <span class="emphasis"><em>F</em></span> is the fraction of the calculation that must be executed serially, then Amdahl’s law says that on a machine with <span class="emphasis"><em>N</em></span> processors, we can achieve a speedup of at most:</p>
<div class="mediaobject"><img src="graphics/225equ01.gif" height="71" alt="Amdahl’s Law"/></div>
<p class="continued">As <span class="emphasis"><em>N</em></span> approaches infinity, the maximum speedup converges to 1/<span class="emphasis"><em>F</em></span>, meaning that a program in which fifty percent of the processing must be executed serially can be sped up only by a factor of two, regardless of how many processors are available, and a program in which ten percent must be executed serially can be sped up by at most a factor of ten. Amdahl’s law also quantifies the efficiency cost of serialization. With ten processors, a program with 10%serialization can achieve at most a speedup of 5.3 (at 53% utilization), and with 100 processors it can achieve at most a speedup of 9.2 (at 9% utilization). It takes a lot of inefficiently utilized CPUs to never get to that factor of ten.</p>
<p><a class="link" href="ch11s02.html#ch11fig01" title="Figure 11.1. Maximum Utilization Under Amdahl’s Law for Various Serialization Percentages.">Figure 11.1</a> shows the maximum possible processor utilization for varying degrees of serial execution and numbers of processors. (Utilization is defined as the speedup divided by the number of processors.) It is clear that as processor counts increase, even a small percentage of serialized execution limits how much throughput can be increased with additional computing resources.</p>
<div class="figure-float"><div class="figure"><a id="ch11fig01"/><p class="title"><b>Figure 11.1. Maximum Utilization Under Amdahl’s Law for Various Serialization Percentages.</b></p><div class="figure-contents">


<div class="mediaobject"><img src="graphics/11fig01.gif" height="306" alt="Maximum Utilization Under Amdahl’s Law for Various Serialization Percentages."/></div>
</div></div><br class="figure-break"/></div>
<p><a class="link" href="ch06.html" title="Chapter 6. Task Execution">Chapter 6</a> explored identifying logical boundaries for decomposing applications into tasks. But in order to predict what kind of speedup is possible from running your application on a multiprocessor system, you also need to identify the sources of serialization in your tasks.</p>
<p><a id="iddle1777" class="indexterm"/><a id="iddle1778" class="indexterm"/><a id="iddle3986" class="indexterm"/><a id="iddle3987" class="indexterm"/><a id="iddle4229" class="indexterm"/><a id="iddle4230" class="indexterm"/><a id="iddle4272" class="indexterm"/><a id="iddle4273" class="indexterm"/><a id="iddle5034" class="indexterm"/>Imagine an application where <span class="emphasis"><em>N</em></span> threads execute <code class="literal">doWork</code> in <a class="link" href="ch11s02.html#ch11list01" title="Example 11.1. Serialized Access to a Task Queue.">Listing 11.1</a>, fetching tasks from a shared work queue and processing them; assume that tasks do not depend on the results or side effects of other tasks. Ignoring for a moment how the tasks get onto the queue, how well will this application scale as we add processors? At first glance, it may appear that the application is completely parallelizable: tasks do not wait for each other, and the more processors available, the more tasks can be processed concurrently. However, there is a serial component as well—fetching the task from the work queue. The work queue is shared by all the worker threads, and it will require some amount of synchronization to maintain its integrity in the face of concurrent access. If locking is used to guard the state of the queue, then while one thread is dequeing a task, other threads that need to dequeue their next task must wait—and this is where task processing is serialized.</p>
<p>The processing time of a single task includes not only the time to execute the task <code class="literal">Runnable</code>, but also the time to dequeue the task from the shared work queue. If the work queue is a <code class="literal">LinkedBlockingQueue</code>, the dequeue operation may block less than with a synchronized <code class="literal">LinkedList</code> because <code class="literal">LinkedBlockingQueue</code> uses a more scalable algorithm, but accessing any shared data structure fundamentally introduces an element of serialization into a program.</p>
<p>This example also ignores another common source of serialization: result handling. All useful computations produce some sort of result or side effect—if not, they can be eliminated as dead code. Since <code class="literal">Runnable</code> provides for no explicit result handling, these tasks must have some sort of side effect, say writing their results to a log file or putting them in a data structure. Log files and result containers are usually shared by multiple worker threads and therefore are also a <a id="iddle1028" class="indexterm"/><a id="iddle1029" class="indexterm"/><a id="iddle1082" class="indexterm"/><a id="iddle2220" class="indexterm"/><a id="iddle2410" class="indexterm"/><a id="iddle2579" class="indexterm"/><a id="iddle3762" class="indexterm"/><a id="iddle3763" class="indexterm"/><a id="iddle4078" class="indexterm"/><a id="iddle4079" class="indexterm"/><a id="iddle4176" class="indexterm"/><a id="iddle4872" class="indexterm"/><a id="iddle4873" class="indexterm"/>source of serialization. If instead each thread maintains its own data structure for results that are merged after all the tasks are performed, then the final merge is a source of serialization.</p>
<div class="example"><a id="ch11list01"/><p class="title"><b>Example 11.1. Serialized Access to a Task Queue.</b></p><div class="example-contents">

<pre class="programlisting">public class WorkerThread extends Thread {
    private final BlockingQueue&lt;Runnable&gt; queue;

    public WorkerThread(BlockingQueue&lt;Runnable&gt; queue) {
        this.queue = queue;
    }

    public void run() {
        while (true) {
            try {
                Runnable task = queue.take();
                task.run();
            } catch (InterruptedException e) {
                break;  /*  Allow thread to exit  */
            }
        }
    }
}
</pre>
</div></div><br class="example-break"/>
<div class="sidebar"><a id="ch11sb04"/><p class="title"><b/></p>
<p>All concurrent applications have some sources of serialization; if you think yours does not, look again.</p>
</div>
<div class="section" title="Example: Serialization Hidden in Frameworks"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lev2sec3"/>Example: Serialization Hidden in Frameworks</h3></div></div></div>

<p>To see how serialization can be hidden in the structure of an application, we can compare throughput as threads are added and infer differences in serialization based on observed differences in scalability. <a class="link" href="ch11s02.html#ch11fig02" title="Figure 11.2. Comparing Queue Implementations.">Figure 11.2</a> shows a simple application in which multiple threads repeatedly remove an element from a shared <code class="literal">Queue</code> and process it, similar to <a class="link" href="ch11s02.html#ch11list01" title="Example 11.1. Serialized Access to a Task Queue.">Listing 11.1</a>. The processing step involves only thread-local computation. If a thread finds the queue is empty, it puts a batch of new elements on the queue so that other threads have something to process on their next iteration. Accessing the shared queue clearly entails some degree of serialization, but the processing step is entirely parallelizable since it involves no shared data.</p>
<div class="figure-float"><div class="figure"><a id="ch11fig02"/><p class="title"><b>Figure 11.2. Comparing Queue Implementations.</b></p><div class="figure-contents">


<div class="mediaobject"><img src="graphics/11fig02.gif" height="255" alt="Comparing Queue Implementations."/></div>
</div></div><br class="figure-break"/></div>
<p>The curves in <a class="link" href="ch11s02.html#ch11fig02" title="Figure 11.2. Comparing Queue Implementations.">Figure 11.2</a> compare throughput for two thread-safe <code class="literal">Queue</code> implementations: a <code class="literal">LinkedList</code> wrapped with <code class="literal">synchronizedList</code>, and a <code class="literal">ConcurrentLinkedQueue</code>. The tests were run on an 8-way Sparc V880 system running <a id="iddle1608" class="indexterm"/><a id="iddle1620" class="indexterm"/><a id="iddle2471" class="indexterm"/><a id="iddle2472" class="indexterm"/><a id="iddle4082" class="indexterm"/><a id="iddle4177" class="indexterm"/><a id="iddle4178" class="indexterm"/><a id="iddle4182" class="indexterm"/>Solaris. While each run represents the same amount of “work”, we can see that merely changing queue implementations can have a big impact on scalability.</p>
<p>The throughput of <code class="literal">ConcurrentLinkedQueue</code> continues to improve until it hits the number of processors and then remains mostly constant. On the other hand, the throughput of the synchronized <code class="literal">LinkedList</code> shows some improvement up to three threads, but then falls off as synchronization overhead increases. By the time it gets to four or five threads, contention is so heavy that every access to the queue lock is contended and throughput is dominated by context switching.</p>
<p>The difference in throughput comes from differing degrees of serialization between the two queue implementations. The synchronized <code class="literal">LinkedList</code> guards the entire queue state with a single lock that is held for the duration of the <code class="literal">offer</code> or <code class="literal">remove</code> call; <code class="literal">ConcurrentLinkedQueue</code> uses a sophisticated nonblocking queue algorithm (see <a class="link" href="ch15s04.html#ch15lev2sec7" title="A Nonblocking Linked List">Section 15.4.2</a>) that uses atomic references to update individual link pointers. In one, the entire insertion or removal is serialized; in the other, only updates to individual pointers are serialized.</p>
</div>
<div class="section" title="Applying Amdahl’s Law Qualitatively"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lev2sec4"/>Applying Amdahl’s Law Qualitatively</h3></div></div></div>

<p>Amdahl’s law quantifies the possible speedup when more computing resources are available, if we can accurately estimate the fraction of execution that is serialized. Although measuring serialization directly can be difficult, Amdahl’s law can still be useful without such measurement.</p>
<p>Since our mental models are influenced by our environment, many of us are used to thinking that a multiprocessor system has two or four processors, or maybe (if we’ve got a big budget) as many as a few dozen, because this is the technology that has been widely available in recent years. But as multicore CPUs <a id="iddle1286" class="indexterm"/><a id="iddle1287" class="indexterm"/><a id="iddle1611" class="indexterm"/><a id="iddle1612" class="indexterm"/><a id="iddle1692" class="indexterm"/><a id="iddle2467" class="indexterm"/><a id="iddle2468" class="indexterm"/><a id="iddle3101" class="indexterm"/><a id="iddle3102" class="indexterm"/><a id="iddle3147" class="indexterm"/><a id="iddle3152" class="indexterm"/><a id="iddle3887" class="indexterm"/><a id="iddle3888" class="indexterm"/><a id="iddle4058" class="indexterm"/><a id="iddle4349" class="indexterm"/><a id="iddle4501" class="indexterm"/><a id="iddle4762" class="indexterm"/>become mainstream, systems will have hundreds or even thousands of processors. <sup>[<a id="ch11fn03" href="#ftn.ch11fn03" class="footnote">3</a>]</sup> Algorithms that seem scalable on a four-way system may have hidden scalability bottlenecks that have just not yet been encountered.</p>
<p>When evaluating an algorithm, thinking “in the limit” about what would happen with hundreds or thousands of processors can offer some insight into where scaling limits might appear. For example, <a class="link" href="ch11s04.html#ch11lev2sec9" title="Reducing Lock Granularity">Sections 11.4.2</a> and <a class="link" href="ch11s04.html#ch11lev2sec10" title="Lock Striping">11.4.3</a> discuss two techniques for reducing lock granularity: lock splitting (splitting one lock into two) and lock striping (splitting one lock into many). Looking at them through the lens of Amdahl’s law, we see that splitting a lock in two does not get us very far towards exploiting many processors, but lock striping seems much more promising because the size of the stripe set can be increased as processor count increases. (Of course, performance optimizations should always be considered in light of actual performance requirements; in some cases, splitting a lock in two may be enough to meet the requirements.)</p>
</div>
<div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.ch11fn03" href="#ch11fn03" class="para">3</a>] </sup>Market update: at this writing, Sun is shipping low-end server systems based on the 8-core Niagara processor, and Azul is shipping high-end server systems (96, 192, and 384-way) based on the 24-core Vega processor.</p></div></div></div></body></html>

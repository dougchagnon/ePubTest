<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Chapter 11. Performance and Scalability</title><link rel="stylesheet" href="styles/convert.css" type="text/css"/></head><body><div class="chapter" title="Chapter 11. Performance and Scalability"><div class="titlepage"><div><div><h1 class="title"><a id="ch11"/>Chapter 11. Performance and Scalability</h1></div></div></div>




<p><a id="iddle1092" class="indexterm"/><a id="iddle1253" class="indexterm"/><a id="iddle1617" class="indexterm"/><a id="iddle1655" class="indexterm"/><a id="iddle1656" class="indexterm"/><a id="iddle1916" class="indexterm"/><a id="iddle1917" class="indexterm"/><a id="iddle3252" class="indexterm"/><a id="iddle3482" class="indexterm"/><a id="iddle3483" class="indexterm"/><a id="iddle3906" class="indexterm"/><a id="iddle3934" class="indexterm"/><a id="iddle3935" class="indexterm"/><a id="iddle3960" class="indexterm"/><a id="iddle3961" class="indexterm"/><a id="iddle4055" class="indexterm"/>One of the primary reasons to use threads is to improve performance.<sup>[<a id="ch11fn01" href="#ftn.ch11fn01" class="footnote">1</a>]</sup> Using threads can improve resource utilization by letting applications more easily exploit available processing capacity, and can improve responsiveness by letting applications begin processing new tasks immediately while existing tasks are still running.</p>
<p>This chapter explores techniques for analyzing, monitoring, and improving the performance of concurrent programs. Unfortunately, many of the techniques for improving performance also increase complexity, thus increasing the likelihood of safety and liveness failures. Worse, some techniques intended to improve performance are actually counterproductive or trade one sort of performance problem for another. While better performance is often desirable—and improving performance can be very satisfying—safety always comes first. First make your program right, then make it fast—and then only if your performance requirements and measurements tell you it needs to be faster. In designing a concurrent application, squeezing out the last bit of performance is often the least of your concerns.</p>



<div class="section" title="Thinking about Performance"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lev1sec1"/>Thinking about Performance</h2></div></div></div>

<p>Improving performance means doing more work with fewer resources. The meaning of “resources” can vary; for a given activity, some specific resource is usually in shortest supply, whether it is CPU cycles, memory, network bandwidth, I/O bandwidth, database requests, disk space, or any number of other resources. When the performance of an activity is limited by availability of a particular resource, we say it is <span class="emphasis"><em>bound</em></span> by that resource: CPU-bound, database-bound, etc.</p>
<p>While the goal may be to improve performance overall, using multiple threads always introduces some performance costs compared to the single-threaded approach. These include the overhead associated with coordinating between threads (locking, signaling, and memory synchronization), increased context switching, <a id="iddle1714" class="indexterm"/><a id="iddle1715" class="indexterm"/><a id="iddle2575" class="indexterm"/><a id="iddle2660" class="indexterm"/><a id="iddle2661" class="indexterm"/><a id="iddle2662" class="indexterm"/><a id="iddle2663" class="indexterm"/><a id="iddle2664" class="indexterm"/><a id="iddle2665" class="indexterm"/><a id="iddle2666" class="indexterm"/><a id="iddle2668" class="indexterm"/><a id="iddle3185" class="indexterm"/><a id="iddle3393" class="indexterm"/><a id="iddle3501" class="indexterm"/><a id="iddle3502" class="indexterm"/><a id="iddle3503" class="indexterm"/><a id="iddle3504" class="indexterm"/><a id="iddle3505" class="indexterm"/><a id="iddle3506" class="indexterm"/><a id="iddle3524" class="indexterm"/><a id="iddle3912" class="indexterm"/><a id="iddle3913" class="indexterm"/><a id="iddle4054" class="indexterm"/><a id="iddle4074" class="indexterm"/><a id="iddle4092" class="indexterm"/><a id="iddle4093" class="indexterm"/><a id="iddle4094" class="indexterm"/><a id="iddle5035" class="indexterm"/><a id="iddle5037" class="indexterm"/><a id="iddle5038" class="indexterm"/>thread creation and teardown, and scheduling overhead. When threading is employed effectively, these costs are more than made up for by greater throughput, responsiveness, or capacity. On the other hand, a poorly designed concurrent application can perform even worse than a comparable sequential one.<sup>[<a id="ch11fn02" href="#ftn.ch11fn02" class="footnote">2</a>]</sup></p>
<p>In using concurrency to achieve better performance, we are trying to do two things: utilize the processing resources we have more effectively, and enable our program to exploit additional processing resources if they become available. From a performance monitoring perspective, this means we are looking to keep the CPUs as busy as possible. (Of course, this doesn’t mean burning cycles with useless computation; we want to keep the CPUs busy with <span class="emphasis"><em>useful</em></span> work.) If the program is compute-bound, then we may be able to increase its capacity by adding more processors; if it can’t even keep the processors we have busy, adding more won’t help. Threading offers a means to keep the CPU(s) “hotter” by decomposing the application so there is always work to be done by an available processor.</p>
<div class="section" title="Performance Versus Scalability"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lev2sec1"/>Performance Versus Scalability</h3></div></div></div>

<p>Application performance can be measured in a number of ways, such as service time, latency, throughput, efficiency, scalability, or capacity. Some of these (service time, latency) aremeasures of “how fast” a given unit of work can be processed or acknowledged; others (capacity, throughput) are measures of “how much” work can be performed with a given quantity of computing resources.</p>
<div class="sidebar"><a id="ch11sb01"/><p class="title"><b/></p>
<p><span class="emphasis"><em>Scalability</em></span> describes the ability to improve throughput or capacity when additional computing resources (such as additional CPUs, memory, storage, or I/O bandwidth) are added.</p>
</div>
<p>Designing and tuning concurrent applications for scalability can be very different from traditional performance optimization. When tuning for performance, the goal is usually to do the <span class="emphasis"><em>same</em></span> work with <span class="emphasis"><em>less</em></span> effort, such as by reusing previously computed results through caching or replacing an <span class="emphasis"><em>O</em></span>(<span class="emphasis"><em>n</em></span><sup>2</sup>) algorithm with an <span class="emphasis"><em>O</em></span>(<span class="emphasis"><em>n</em></span> log <span class="emphasis"><em>n</em></span>) one. When tuning for scalability, you are instead trying to find ways to parallelize the problem so you can take advantage of additional processing resources to do <span class="emphasis"><em>more</em></span> work with <span class="emphasis"><em>more</em></span> resources.</p>
<p>These two aspects of performance—<span class="emphasis"><em>how fast</em></span> and <span class="emphasis"><em>how much</em></span>—are completely separate, and sometimes even at odds with each other. In order to achieve higher scalability or better hardware utilization, we often end up <span class="emphasis"><em>increasing</em></span> the amount of work done to process each <span class="emphasis"><em>individual</em></span> task, such as when we divide tasks into multiple “pipelined” subtasks. Ironically, many of the tricks that improve performance in single-threaded programs are bad for scalability (see <a class="link" href="ch11s04.html#ch11lev2sec11" title="Avoiding Hot Fields">Section 11.4.4</a> for an example).</p>
<p><a id="iddle1695" class="indexterm"/><a id="iddle1696" class="indexterm"/><a id="iddle1918" class="indexterm"/><a id="iddle1919" class="indexterm"/><a id="iddle2058" class="indexterm"/><a id="iddle2562" class="indexterm"/><a id="iddle2667" class="indexterm"/><a id="iddle2943" class="indexterm"/><a id="iddle2944" class="indexterm"/><a id="iddle2945" class="indexterm"/><a id="iddle3236" class="indexterm"/><a id="iddle3237" class="indexterm"/><a id="iddle3391" class="indexterm"/><a id="iddle3525" class="indexterm"/><a id="iddle3546" class="indexterm"/><a id="iddle3547" class="indexterm"/><a id="iddle3884" class="indexterm"/><a id="iddle3885" class="indexterm"/><a id="iddle4048" class="indexterm"/><a id="iddle4049" class="indexterm"/><a id="iddle4077" class="indexterm"/>The familiar three-tier application model—in which presentation, business logic, and persistence are separated and may be handled by different systems—illustrates how improvements in scalability often come at the expense of performance. A monolithic application where presentation, business logic, and persistence are intertwined would almost certainly provide better performance for the <span class="emphasis"><em>first</em></span> unit of work than would a well-factored multitier implementation distributed over multiple systems. How could it not? The monolithic application would not have the network latency inherent in handing off tasks between tiers, nor would it have to pay the costs inherent in separating a computational process into distinct abstracted layers (such as queuing overhead, coordination overhead, and data copying).</p>
<p>However, when the monolithic system reaches its processing capacity, we could have a serious problem: it may be prohibitively difficult to significantly increase capacity. So we often accept the performance costs of longer service time or greater computing resources used per unit of work so that our application can scale to handle greater load by adding more resources.</p>
<p>Of the various aspects of performance, the “how much” aspects—scalability, throughput, and capacity—are usually of greater concern for server applications than the “how fast” aspects. (For interactive applications, latency tends to be more important, so that users need not wait for indications of progress and wonder what is going on.) This chapter focuses primarily on scalability rather than raw single-threaded performance.</p>
</div>
<div class="section" title="Evaluating Performance Tradeoffs"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lev2sec2"/>Evaluating Performance Tradeoffs</h3></div></div></div>

<p>Nearly all engineering decisions involve some form of tradeoff. Using thicker steel in a bridge span may increase its capacity and safety, but also its construction cost. While software engineering decisions don’t usually involve tradeoffs between money and risk to human life, we often have less information with which to make the right tradeoffs. For example, the “quicksort” algorithm is highly efficient for large data sets, but the less sophisticated “bubble sort” is actually more efficient for small data sets. If you are asked to implement an efficient sort routine, you need to know something about the sizes of data sets it will have to process, along with metrics that tell you whether you are trying to optimize average-case time, worst-case time, or predictability. Unfortunately, that information is often not part of the requirements given to the author of a library sort routine. This is one of the reasons why most optimizations are premature: <span class="emphasis"><em>they are often undertaken before a clear set of requirements is available</em></span>.</p>
<div class="sidebar"><a id="ch11sb02"/><p class="title"><b/></p>
<p>Avoid premature optimization. First make it right, then make it fast—<span class="emphasis"><em>if</em></span> it is not already fast enough.</p>
</div>
<p>When making engineering decisions, sometimes you are trading one form of cost for another (service time versus memory consumption); sometimes you are trading cost for safety. Safety doesn’t necessarily mean risk to human lives, as <a id="iddle1083" class="indexterm"/><a id="iddle1084" class="indexterm"/><a id="iddle1085" class="indexterm"/><a id="iddle1177" class="indexterm"/><a id="iddle1200" class="indexterm"/><a id="iddle1462" class="indexterm"/><a id="iddle1463" class="indexterm"/><a id="iddle1687" class="indexterm"/><a id="iddle1688" class="indexterm"/><a id="iddle1689" class="indexterm"/><a id="iddle1690" class="indexterm"/><a id="iddle1691" class="indexterm"/><a id="iddle1823" class="indexterm"/><a id="iddle1824" class="indexterm"/><a id="iddle1825" class="indexterm"/><a id="iddle1826" class="indexterm"/><a id="iddle1827" class="indexterm"/><a id="iddle1829" class="indexterm"/><a id="iddle1830" class="indexterm"/><a id="iddle1891" class="indexterm"/><a id="iddle1892" class="indexterm"/><a id="iddle1893" class="indexterm"/><a id="iddle1894" class="indexterm"/><a id="iddle1895" class="indexterm"/><a id="iddle1954" class="indexterm"/><a id="iddle1955" class="indexterm"/><a id="iddle1956" class="indexterm"/><a id="iddle1957" class="indexterm"/><a id="iddle1958" class="indexterm"/><a id="iddle2055" class="indexterm"/><a id="iddle2056" class="indexterm"/><a id="iddle2057" class="indexterm"/><a id="iddle2362" class="indexterm"/><a id="iddle2385" class="indexterm"/><a id="iddle2386" class="indexterm"/><a id="iddle2387" class="indexterm"/><a id="iddle2388" class="indexterm"/><a id="iddle2389" class="indexterm"/><a id="iddle2390" class="indexterm"/><a id="iddle2450" class="indexterm"/><a id="iddle2451" class="indexterm"/><a id="iddle2452" class="indexterm"/><a id="iddle2453" class="indexterm"/><a id="iddle2454" class="indexterm"/><a id="iddle2455" class="indexterm"/><a id="iddle2498" class="indexterm"/><a id="iddle2499" class="indexterm"/><a id="iddle2500" class="indexterm"/><a id="iddle2501" class="indexterm"/><a id="iddle2553" class="indexterm"/><a id="iddle2554" class="indexterm"/><a id="iddle2564" class="indexterm"/><a id="iddle2565" class="indexterm"/><a id="iddle2630" class="indexterm"/><a id="iddle2671" class="indexterm"/><a id="iddle2750" class="indexterm"/><a id="iddle2751" class="indexterm"/><a id="iddle2752" class="indexterm"/><a id="iddle2753" class="indexterm"/><a id="iddle2754" class="indexterm"/><a id="iddle2755" class="indexterm"/><a id="iddle2867" class="indexterm"/><a id="iddle2868" class="indexterm"/><a id="iddle3166" class="indexterm"/><a id="iddle3183" class="indexterm"/><a id="iddle3184" class="indexterm"/><a id="iddle3193" class="indexterm"/><a id="iddle3246" class="indexterm"/><a id="iddle3247" class="indexterm"/><a id="iddle3248" class="indexterm"/><a id="iddle3249" class="indexterm"/><a id="iddle3250" class="indexterm"/><a id="iddle3344" class="indexterm"/><a id="iddle3392" class="indexterm"/><a id="iddle3469" class="indexterm"/><a id="iddle3470" class="indexterm"/><a id="iddle3567" class="indexterm"/><a id="iddle3568" class="indexterm"/><a id="iddle3569" class="indexterm"/><a id="iddle3570" class="indexterm"/><a id="iddle3571" class="indexterm"/><a id="iddle3606" class="indexterm"/><a id="iddle3690" class="indexterm"/><a id="iddle3703" class="indexterm"/><a id="iddle3704" class="indexterm"/><a id="iddle3705" class="indexterm"/><a id="iddle3742" class="indexterm"/><a id="iddle3880" class="indexterm"/><a id="iddle3881" class="indexterm"/><a id="iddle3898" class="indexterm"/><a id="iddle3899" class="indexterm"/><a id="iddle3900" class="indexterm"/><a id="iddle3901" class="indexterm"/><a id="iddle3902" class="indexterm"/><a id="iddle3903" class="indexterm"/><a id="iddle3916" class="indexterm"/><a id="iddle3917" class="indexterm"/><a id="iddle3918" class="indexterm"/><a id="iddle4005" class="indexterm"/><a id="iddle4006" class="indexterm"/><a id="iddle4010" class="indexterm"/><a id="iddle4011" class="indexterm"/><a id="iddle4012" class="indexterm"/><a id="iddle4120" class="indexterm"/><a id="iddle4121" class="indexterm"/><a id="iddle4282" class="indexterm"/><a id="iddle4337" class="indexterm"/><a id="iddle4442" class="indexterm"/><a id="iddle4451" class="indexterm"/><a id="iddle4452" class="indexterm"/><a id="iddle4453" class="indexterm"/><a id="iddle4454" class="indexterm"/><a id="iddle4455" class="indexterm"/><a id="iddle4667" class="indexterm"/><a id="iddle4668" class="indexterm"/><a id="iddle4669" class="indexterm"/><a id="iddle4687" class="indexterm"/><a id="iddle4688" class="indexterm"/><a id="iddle4689" class="indexterm"/><a id="iddle4690" class="indexterm"/><a id="iddle4691" class="indexterm"/><a id="iddle4692" class="indexterm"/><a id="iddle4937" class="indexterm"/><a id="iddle4938" class="indexterm"/><a id="iddle4945" class="indexterm"/><a id="iddle5109" class="indexterm"/><a id="iddle5110" class="indexterm"/>it did in the bridge example. Many performance optimizations come at the cost of readability or maintainability—the more “clever” or nonobvious code is, the harder it is to understand and maintain. Sometimes optimizations entail compromising good object-oriented design principles, such as breaking encapsulation; sometimes they involve greater risk of error, because faster algorithms are usually more complicated. (If you can’t spot the costs or risks, you probably haven’t thought it through carefully enough to proceed.)</p>
<p>Most performance decisions involve multiple variables and are highly situational. Before deciding that one approach is “faster” than another, ask yourself some questions:</p>
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>What do you mean by “faster”?</p></li><li class="listitem"><p>Under what conditions will this approach <span class="emphasis"><em>actually</em></span> be faster? Under light or heavy load? With large or small data sets? Can you support your answer with measurements?</p></li><li class="listitem"><p>How often are these conditions likely to arise in your situation? Can you support your answer with measurements?</p></li><li class="listitem"><p>Is this code likely to be used in other situations where the conditions may be different?</p></li><li class="listitem"><p>What hidden costs, such as increased development or maintenance risk, are you trading for this improved performance? Is this a good tradeoff?</p></li></ul></div>
<p class="continued">These considerations apply to any performance-related engineering decision, but this is a book about concurrency. Why are we recommending such a conservative approach to optimization? <span class="emphasis"><em>The quest for performance is probably the single greatest source of concurrency bugs.</em></span> The belief that synchronization was “too slow” led to many clever-looking but dangerous idioms for reducing synchronization (such as double-checked locking, discussed in <a class="link" href="ch16s02.html#ch16lev2sec8" title="Double-checked Locking">Section 16.2.4</a>), and is often cited as an excuse for not following the rules regarding synchronization. Because concurrency bugs are among the most difficult to track down and eliminate, however, anything that risks introducing them must be undertaken very carefully.</p>
<p>Worse, when you trade safety for performance, you may get neither. Especially when it comes to concurrency, the intuition of many developers about where a performance problem lies or which approach will be faster or more scalable is often incorrect. It is therefore imperative that any performance tuning exercise be accompanied by concrete performance requirements (so you know both when to tune and when to <span class="emphasis"><em>stop</em></span> tuning) and with a measurement program in place using a realistic configuration and load profile. Measure again after tuning to verify that you’ve achieved the desired improvements. The safety and maintenance risks associated with many optimizations are bad enough—you don’t want to pay these costs if you don’t need to—and you definitely don’t want to pay them if you don’t even get the desired benefit.</p>
<div class="sidebar"><a id="ch11sb03"/><p class="title"><b/></p>
<p>Measure, don’t guess.</p>
</div>
<p><a id="iddle1077" class="indexterm"/><a id="iddle1078" class="indexterm"/><a id="iddle1079" class="indexterm"/><a id="iddle1080" class="indexterm"/><a id="iddle3186" class="indexterm"/><a id="iddle3189" class="indexterm"/><a id="iddle3190" class="indexterm"/><a id="iddle3389" class="indexterm"/><a id="iddle3390" class="indexterm"/><a id="iddle3456" class="indexterm"/><a id="iddle3457" class="indexterm"/><a id="iddle3472" class="indexterm"/><a id="iddle3513" class="indexterm"/><a id="iddle3694" class="indexterm"/><a id="iddle3939" class="indexterm"/><a id="iddle3940" class="indexterm"/><a id="iddle4180" class="indexterm"/><a id="iddle4181" class="indexterm"/><a id="iddle4185" class="indexterm"/><a id="iddle4186" class="indexterm"/><a id="iddle4652" class="indexterm"/><a id="iddle4653" class="indexterm"/><a id="iddle4951" class="indexterm"/><a id="iddle5030" class="indexterm"/><a id="iddle5031" class="indexterm"/><a id="iddle5032" class="indexterm"/><a id="iddle5033" class="indexterm"/>There are sophisticated profiling tools on the market for measuring performance and tracking down performance bottlenecks, but you don’t have to spend a lot of money to figure out what your program is doing. For example, the free <code class="literal">perfbar</code> application can give you a good picture of how busy the CPUs are, and since your goal is usually to keep the CPUs busy, this is a very good way to evaluate whether you need performance tuning or how effective your tuning has been.</p>
</div>
</div>










<div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.ch11fn01" href="#ch11fn01" class="para">1</a>] </sup>Some might argue this is the <span class="emphasis"><em>only</em></span> reason we put up with the complexity threads introduce.</p></div><div class="footnote"><p><sup>[<a id="ftn.ch11fn02" href="#ch11fn02" class="para">2</a>] </sup>A colleague provided this amusing anecodote: he had been involved in the testing of an expensive and complex application that managed its work via a tunable thread pool. After the system was complete, testing showed that the optimal number of threads for the pool was . . . 1. This should have been obvious from the outset; the target system was a single-CPU system and the application was almost entirely CPU-bound.</p></div></div></div></body></html>

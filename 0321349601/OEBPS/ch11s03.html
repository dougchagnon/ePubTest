<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Costs Introduced by Threads</title><link rel="stylesheet" href="styles/convert.css" type="text/css"/></head><body><div class="section" title="Costs Introduced by Threads"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lev1sec3"/>Costs Introduced by Threads</h2></div></div></div>

<p>Single-threaded programs incur neither scheduling nor synchronization overhead, and need not use locks to preserve the consistency of data structures. Scheduling and interthread coordination have performance costs; for threads to offer a performance improvement, the performance benefits of parallelization must outweigh the costs introduced by concurrency.</p>
<div class="section" title="Context Switching"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lev2sec5"/>Context Switching</h3></div></div></div>

<p>If the main thread is the only schedulable thread, it will almost never be scheduled out. On the other hand, if there are more runnable threads than CPUs, eventually the OS will preempt one thread so that another can use the CPU. This causes a <span class="emphasis"><em>context switch</em></span>, which requires saving the execution context of the currently running thread and restoring the execution context of the newly scheduled thread.</p>
<p>Context switches are not free; thread scheduling requires manipulating shared data structures in the OS and JVM. The OS and JVMuse the same CPUs your program does; more CPU time spent in JVM and OS code means less is available for your program. But OS and JVM activity is not the only cost of context switches. When a new thread is switched in, the data it needs is unlikely to be in the local processor cache, so a context switch causes a flurry of cache misses, and thus threads run a little more slowly when they are first scheduled. This is one of the reasons that schedulers give each runnable thread a certain minimum time quantum even when many other threads are waiting: it amortizes the cost of the context switch and its consequences over more uninterrupted execution time, improving overall throughput (at some cost to responsiveness).</p>

<p/><div class="example"><a id="ch11list02"/><p class="title"><b>Example 11.2. Synchronization that has No Effect. <span class="emphasis"><em>Don’t Do this.</em></span></b></p><div class="example-contents">


<div class="mediaobject"><img src="graphics/face.jpg" width="52" alt="Synchronization that has No Effect. Don’t Do this."/></div>
<pre class="programlisting">synchronized (new Object()) {
    <span class="emphasis"><em>// do something</em></span>
}
</pre>
</div></div><br class="example-break"/>
<p><a id="iddle1088" class="indexterm"/><a id="iddle1195" class="indexterm"/><a id="iddle1212" class="indexterm"/><a id="iddle1281" class="indexterm"/><a id="iddle1282" class="indexterm"/><a id="iddle1604" class="indexterm"/><a id="iddle2047" class="indexterm"/><a id="iddle2334" class="indexterm"/><a id="iddle2916" class="indexterm"/><a id="iddle2925" class="indexterm"/><a id="iddle3194" class="indexterm"/><a id="iddle3206" class="indexterm"/><a id="iddle3207" class="indexterm"/><a id="iddle3476" class="indexterm"/><a id="iddle3509" class="indexterm"/><a id="iddle3854" class="indexterm"/><a id="iddle3855" class="indexterm"/><a id="iddle4546" class="indexterm"/><a id="iddle4554" class="indexterm"/><a id="iddle4558" class="indexterm"/><a id="iddle4559" class="indexterm"/><a id="iddle4579" class="indexterm"/><a id="iddle4946" class="indexterm"/><a id="iddle5006" class="indexterm"/><a id="iddle5007" class="indexterm"/><a id="iddle5112" class="indexterm"/>When a thread blocks because it is waiting for a contended lock, the JVM usually suspends the thread and allows it to be switched out. If threads block frequently, they will be unable to use their full scheduling quantum. A program that does more blocking (blocking I/O, waiting for contended locks, or waiting on condition variables) incurs more context switches than one that is CPU-bound, increasing scheduling overhead and reducing throughput. (Nonblocking algorithms can also help reduce context switches; see <a class="link" href="ch15.html" title="Chapter 15. Atomic Variables and Nonblocking Synchronization">Chapter 15</a>.)</p>
<p>The actual cost of context switching varies across platforms, but a good rule of thumb is that a context switch costs the equivalent of 5,000 to 10,000 clock cycles, or several microseconds on most current processors.</p>
<p>The <code class="literal">vmstat</code> command on Unix systems and the <code class="literal">perfmon</code> tool on Windows systems report the number of context switches and the percentage of time spent in the kernel. High kernel usage (over 10%) often indicates heavy scheduling activity, which may be caused by blocking due to I/O or lock contention.</p>
</div>
<div class="section" title="Memory Synchronization"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lev2sec6"/>Memory Synchronization</h3></div></div></div>

<p>The performance cost of synchronization comes from several sources. The visibility guarantees provided by <code class="literal">synchronized</code> and <code class="literal">volatile</code> may entail using special instructions called <span class="emphasis"><em>memory barriers</em></span> that can flush or invalidate caches, flush hardware write buffers, and stall execution pipelines. Memory barriers may also have indirect performance consequences because they inhibit other compiler optimizations; most operations cannot be reordered with memory barriers.</p>
<p>When assessing the performance impact of synchronization, it is important to distinguish between <span class="emphasis"><em>contended</em></span> and <span class="emphasis"><em>uncontended</em></span> synchronization. The <code class="literal">synchronized</code> mechanism is optimized for the uncontended case (<code class="literal">volatile</code> is always uncontended), and at this writing, the performance cost of a “fast-path” uncontended synchronization ranges from 20 to 250 clock cycles for most systems. While this is certainly not zero, the effect of needed, uncontended synchronization is rarely significant in overall application performance, and the alternative involves compromising safety and potentially signing yourself (or your successor) up for some very painful bug hunting later.</p>
<p>Modern JVMs can reduce the cost of incidental synchronization by optimizing away locking that can be proven never to contend. If a lock object is accessible only to the current thread, the JVM is permitted to optimize away a lock acquisition because there is no way another thread could synchronize on the same lock. For example, the lock acquisition in <a class="link" href="ch11s03.html#ch11list02" title="Example 11.2. Synchronization that has No Effect. Don’t Do this.">Listing 11.2</a> can always be eliminated by the JVM.</p>
<p>More sophisticated JVMs can use <span class="emphasis"><em>escape analysis</em></span> to identify when a local object reference is never published to the heap and is therefore thread-local. In <code class="literal">getStoogeNames</code> <a id="iddle1057" class="indexterm"/><a id="iddle1182" class="indexterm"/><a id="iddle1183" class="indexterm"/><a id="iddle1366" class="indexterm"/><a id="iddle1995" class="indexterm"/><a id="iddle1996" class="indexterm"/><a id="iddle2550" class="indexterm"/><a id="iddle2560" class="indexterm"/><a id="iddle2561" class="indexterm"/><a id="iddle3071" class="indexterm"/><a id="iddle3090" class="indexterm"/><a id="iddle3304" class="indexterm"/><a id="iddle3386" class="indexterm"/><a id="iddle3387" class="indexterm"/>in <a class="link" href="ch11s03.html#ch11list03" title="Example 11.3. Candidate for Lock Elision.">Listing 11.3</a>, the only reference to the <code class="literal">List</code> is the local variable <code class="literal">stooges</code>, and stack-confined variables are automatically thread-local. A naive execution of <code class="literal">getStoogeNames</code> would acquire and release the lock on the <code class="literal">Vector</code> four times, once for each call to <code class="literal">add</code> or <code class="literal">toString</code>. However, a smart runtime compiler can inline these calls and then see that <code class="literal">stooges</code> and its internal state never escape, and therefore that all four lock acquisitions can be eliminated.<sup>[<a id="ch11fn04" href="#ftn.ch11fn04" class="footnote">4</a>]</sup></p>
<div class="example"><a id="ch11list03"/><p class="title"><b>Example 11.3. Candidate for Lock Elision.</b></p><div class="example-contents">

<pre class="programlisting">public String getStoogeNames() {
    List&lt;String&gt; stooges = new Vector&lt;String&gt;();
    stooges.add("Moe");
    stooges.add("Larry");
    stooges.add("Curly");
    return stooges.toString();
}
</pre>
</div></div><br class="example-break"/>
<p>Even without escape analysis, compilers can also perform <span class="emphasis"><em>lock coarsening</em></span>, the merging of adjacent <code class="literal">synchronized</code> blocks using the same lock. For <code class="literal">getStooge-Names</code>, a JVM that performs lock coarsening might combine the three calls to <code class="literal">add</code> and the call to <code class="literal">toString</code> into a single lock acquisition and release, using heuristics on the relative cost of synchronization versus the instructions inside the <code class="literal">synchronized</code> block.<sup>[<a id="ch11fn05" href="#ftn.ch11fn05" class="footnote">5</a>]</sup> Not only does this reduce the synchronization overhead, but it also gives the optimizer a much larger block to work with, likely enabling other optimizations.</p>
<div class="sidebar"><a id="ch11sb05"/><p class="title"><b/></p>
<p>Don’t worry excessively about the cost of uncontended synchronization. The basic mechanism is already quite fast, and JVMs can perform additional optimizations that further reduce or eliminate the cost. Instead, focus optimization efforts on areas where lock contention actually occurs.</p>
</div>
<p>Synchronization by one thread can also affect the performance of other threads. Synchronization creates traffic on the shared memory bus; this bus has a limited bandwidth and is shared across all processors. If threads must compete for synchronization bandwidth, all threads using synchronization will suffer.<sup>[<a id="ch11fn06" href="#ftn.ch11fn06" class="footnote">6</a>]</sup></p>
</div>
<div class="section" title="Blocking"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lev2sec7"/>Blocking</h3></div></div></div>


<p><a id="iddle1227" class="indexterm"/><a id="iddle1233" class="indexterm"/><a id="iddle1593" class="indexterm"/><a id="iddle1594" class="indexterm"/><a id="iddle2576" class="indexterm"/><a id="iddle3009" class="indexterm"/><a id="iddle3010" class="indexterm"/><a id="iddle3082" class="indexterm"/><a id="iddle3083" class="indexterm"/><a id="iddle3388" class="indexterm"/><a id="iddle3516" class="indexterm"/><a id="iddle4065" class="indexterm"/><a id="iddle4066" class="indexterm"/><a id="iddle4341" class="indexterm"/><a id="iddle4342" class="indexterm"/><a id="iddle4510" class="indexterm"/><a id="iddle4511" class="indexterm"/><a id="iddle4825" class="indexterm"/><a id="iddle4826" class="indexterm"/><a id="iddle5142" class="indexterm"/>Uncontended synchronization can be handled entirely within the JVM (<a class="link" href="bi01.html#biblio01_002" title="Thin Locks: Featherweight Synchronization for Java">Bacon et al., 1998</a>); contended synchronization may require OS activity, which adds to the cost. When locking is contended, the losing thread(s) must block. The JVM can implement blocking either via <span class="emphasis"><em>spin-waiting</em></span> (repeatedly trying to acquire the lock until it succeeds) or by <span class="emphasis"><em>suspending</em></span> the blocked thread through the operating system. Which is more efficient depends on the relationship between context switch overhead and the time until the lock becomes available; spin-waiting is preferable for short waits and suspension is preferable for long waits. Some JVMs choose between the two adaptively based on profiling data of past wait times, but most just suspend threads waiting for a lock.</p>
<p>Suspending a thread because it could not get a lock, or because it blocked on a condition wait or blocking I/O operation, entails two additional context switches and all the attendant OS and cache activity: the blocked thread is switched out before its quantum has expired, and is then switched back in later after the lock or other resource becomes available. (Blocking due to lock contention also has a cost for the thread holding the lock: when it releases the lock, it must then ask the OS to resume the blocked thread.)</p>
</div>
<div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.ch11fn04" href="#ch11fn04" class="para">4</a>] </sup>This compiler optimization, called <span class="emphasis"><em>lock elision</em></span>, is performed by the IBM JVM and is expected in HotSpot as of Java 7.</p></div><div class="footnote"><p><sup>[<a id="ftn.ch11fn05" href="#ch11fn05" class="para">5</a>] </sup>A smart dynamic compiler can figure out that this method always returns the same string, and after the first execution recompile <code class="literal">getStoogeNames</code> to simply return the value returned by the first execution.</p></div><div class="footnote"><p><sup>[<a id="ftn.ch11fn06" href="#ch11fn06" class="para">6</a>] </sup>This aspect is sometimes used to argue against the use of nonblocking algorithms without some sort of backoff, because under heavy contention, nonblocking algorithms generate more synchronization traffic than lock-based ones. See <a class="link" href="ch15.html" title="Chapter 15. Atomic Variables and Nonblocking Synchronization">Chapter 15</a>.</p></div></div></div></body></html>

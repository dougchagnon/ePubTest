<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Reducing Lock Contention</title><link rel="stylesheet" href="styles/convert.css" type="text/css"/></head><body><div class="section" title="Reducing Lock Contention"><div class="titlepage"><div><div><h2 class="title"><a id="ch11lev1sec4"/>Reducing Lock Contention</h2></div></div></div>

<p>We’ve seen that serialization hurts scalability and that context switches hurt performance. Contended locking causes both, so reducing lock contention can improve both performance and scalability.</p>
<p>Access to resources guarded by an exclusive lock is serialized—only one thread at a time may access it. Of course, we use locks for good reasons, such as preventing data corruption, but this safety comes at a price. Persistent contention for a lock limits scalability.</p>
<div class="sidebar"><a id="ch11sb06"/><p class="title"><b/></p>
<p>The principal threat to scalability in concurrent applications is the exclusive resource lock.</p>
</div>
<p>Two factors influence the likelihood of contention for a lock: how often that lock is requested and how long it is held once acquired.<sup>[<a id="ch11fn07" href="#ftn.ch11fn07" class="footnote">7</a>]</sup> If the product of these factors is sufficiently small, then most attempts to acquire the lock will be uncontended, and lock contention will not pose a significant scalability impediment. If, however, the lock is in sufficiently high demand, threads will block waiting for it; in the extreme case, processors will sit idle even though there is plenty of work to do.</p>
<div class="sidebar"><a id="ch11sb07"/><p class="title"><b/></p>
<p><a id="iddle2078" class="indexterm"/><a id="iddle2548" class="indexterm"/><a id="iddle2549" class="indexterm"/><a id="iddle3080" class="indexterm"/><a id="iddle3145" class="indexterm"/><a id="iddle3290" class="indexterm"/><a id="iddle3291" class="indexterm"/><a id="iddle3292" class="indexterm"/><a id="iddle4071" class="indexterm"/><a id="iddle4109" class="indexterm"/><a id="iddle4110" class="indexterm"/><a id="iddle4907" class="indexterm"/><a id="iddle4908" class="indexterm"/>There are three ways to reduce lock contention:</p>
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p>Reduce the duration for which locks are held;</p></li><li class="listitem"><p>Reduce the frequency with which locks are requested; or</p></li><li class="listitem"><p>Replace exclusive locks with coordination mechanisms that permit greater concurrency.</p></li></ul></div>
</div>
<div class="section" title="Narrowing Lock Scope (“Get in, Get Out”)"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lev2sec8"/>Narrowing Lock Scope (“Get in, Get Out”)</h3></div></div></div>

<p>An effective way to reduce the likelihood of contention is to hold locks as briefly as possible. This can be done by moving code that doesn’t require the lock out of <code class="literal">synchronized</code> blocks, especially for expensive operations and potentially blocking operations such as I/O.</p>
<p>It is easy to see how holding a “hot” lock for too long can limit scalability; we saw an example of this in <code class="literal">SynchronizedFactorizer</code> in <a class="link" href="ch02.html" title="Chapter 2. Thread Safety">Chapter 2</a>. If an operation holds a lock for 2 milliseconds and every operation requires that lock, throughput can be no greater than 500 operations per second, no matter how many processors are available. Reducing the time the lock is held to 1 millisecond improves the lock-induced throughput limit to 1000 operations per second.<sup>[<a id="ch11fn08" href="#ftn.ch11fn08" class="footnote">8</a>]</sup></p>
<p><code class="literal">AttributeStore</code> in <a class="link" href="ch11s04.html#ch11list04" title="Example 11.4. Holding a Lock Longer than Necessary.">Listing 11.4</a> shows an example of holding a lock longer than necessary. The <code class="literal">userLocationMatches</code> method looks up the user’s location in a <code class="literal">Map</code> and uses regular expression matching to see if the resulting value matches the supplied pattern. The entire <code class="literal">userLocationMatches</code> method is <code class="literal">synchronized</code>, but the only portion of the code that actually needs the lock is the call to <code class="literal">Map.get</code>.</p>
<div class="example"><a id="ch11list04"/><p class="title"><b>Example 11.4. Holding a Lock Longer than Necessary.</b></p><div class="example-contents">

<div class="mediaobject"><img src="graphics/face1.jpg" width="45" alt="Holding a Lock Longer than Necessary."/></div>
<pre class="programlisting">@ThreadSafe
public class AttributeStore {
    @GuardedBy("this") private final Map&lt;String, String&gt;
            attributes = new HashMap&lt;String, String&gt;();

    public <span class="strong"><strong>synchronized</strong></span>  boolean userLocationMatches(String name,
                                                     String regexp) {
        String key = "users." + name + ".location";
        String location = attributes.get(key);
        if (location == null)
            return false;
        else
            return Pattern.matches(regexp, location);
    }
}
</pre>
</div></div><br class="example-break"/>
<p><a id="iddle1081" class="indexterm"/><a id="iddle1161" class="indexterm"/><a id="iddle1162" class="indexterm"/><a id="iddle1860" class="indexterm"/><a id="iddle1861" class="indexterm"/><a id="iddle2082" class="indexterm"/><a id="iddle4491" class="indexterm"/><a id="iddle4735" class="indexterm"/><code class="literal">BetterAttributeStore</code> in <a class="link" href="ch11s04.html#ch11list05" title="Example 11.5. Reducing Lock Duration.">Listing 11.5</a> rewrites <code class="literal">AttributeStore</code> to reduce significantly the lock duration. The first step is to construct the <code class="literal">Map</code> key associated with the user’s location, a string of the form <code class="literal">users.</code><span class="emphasis"><em>name</em></span><code class="literal">.location</code>. This entails instantiating a <code class="literal">StringBuilder</code> object, appending several strings to it, and instantiating the result as a <code class="literal">String</code>. After the location has been retrieved, the regular expression is matched against the resulting location string. Because constructing the key string and processing the regular expression do not access shared state, they need not be executed with the lock held. <code class="literal">BetterAttributeStore</code> factors these steps out of the <code class="literal">synchronized</code> block, thus reducing the time the lock is held.</p>
<div class="example"><a id="ch11list05"/><p class="title"><b>Example 11.5. Reducing Lock Duration.</b></p><div class="example-contents">

<pre class="programlisting">@ThreadSafe
public class BetterAttributeStore {
    @GuardedBy("this") private final Map&lt;String, String&gt;
            attributes = new HashMap&lt;String, String&gt;();

    public boolean userLocationMatches(String name, String regexp) {
        String key = "users." + name + ".location";
        String location;
        <span class="strong"><strong>synchronized (this)</strong></span> {
            location = attributes.get(key);
        }
        if (location == null)
            return false;
        else
            return Pattern.matches(regexp, location);
    }
}
</pre>
</div></div><br class="example-break"/>
<p>Reducing the scope of the lock in <code class="literal">userLocationMatches</code> substantially reduces the number of instructions that are executed with the lock held. By Amdahl’s law, this removes an impediment to scalability because the amount of serialized code is reduced.</p>
<p>Because <code class="literal">AttributeStore</code> has only one state variable, <code class="literal">attributes</code>, we can improve it further by the technique of <span class="emphasis"><em>delegating thread safety</em></span> (<a class="link" href="ch04s03.html" title="Delegating Thread Safety">Section 4.3</a>). By replacing <code class="literal">attributes</code> with a thread-safe <code class="literal">Map</code> (a <code class="literal">Hashtable</code>, <code class="literal">synchronizedMap</code>, or <code class="literal">ConcurrentHashMap</code>), <code class="literal">AttributeStore</code> can delegate all its thread safety obligations to the underlying thread-safe collection. This eliminates the need for explicit synchronization in <code class="literal">AttributeStore</code>, reduces the lock scope to the duration of the <code class="literal">Map</code> access, and removes the risk that a future maintainer will undermine thread safety by forgetting to acquire the appropriate lock before accessing <code class="literal">attributes</code>.</p>
<p>While shrinking <code class="literal">synchronized</code> blocks can improve scalability, a <code class="literal">synchronized</code> block can be <span class="emphasis"><em>too</em></span> small—operations that need to be atomic (such updating multiple variables that participate in an invariant) must be contained in a single <code class="literal">synchronized</code> <a id="iddle1367" class="indexterm"/><a id="iddle1808" class="indexterm"/><a id="iddle2469" class="indexterm"/><a id="iddle2730" class="indexterm"/><a id="iddle3073" class="indexterm"/><a id="iddle3103" class="indexterm"/><a id="iddle3146" class="indexterm"/><a id="iddle3148" class="indexterm"/><a id="iddle4348" class="indexterm"/><a id="iddle4350" class="indexterm"/><a id="iddle4434" class="indexterm"/><a id="iddle4473" class="indexterm"/><a id="iddle5059" class="indexterm"/>block. And because the cost of synchronization is nonzero, breaking one <code class="literal">synchronized</code> block into multiple <code class="literal">synchronized</code> blocks (correctness permitting) at some point becomes counterproductive in terms of performance.<sup>[<a id="ch11fn09" href="#ftn.ch11fn09" class="footnote">9</a>]</sup> The ideal balance is of course platform-dependent, but in practice it makes sense to worry about the size of a <code class="literal">synchronized</code> block only when you can move “substantial” computation or blocking operations out of it.</p>
</div>
<div class="section" title="Reducing Lock Granularity"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lev2sec9"/>Reducing Lock Granularity</h3></div></div></div>

<p>The other way to reduce the fraction of time that a lock is held (and therefore the likelihood that it will be contended) is to have threads ask for it less often. This can be accomplished by <span class="emphasis"><em>lock splitting</em></span> and <span class="emphasis"><em>lock striping</em></span>, which involve using separate locks to guard multiple independent state variables previously guarded by a single lock. These techniques reduce the granularity at which locking occurs, potentially allowing greater scalability—but using more locks also increases the risk of deadlock.</p>
<p>As a thought experiment, imagine what would happen if there was <span class="emphasis"><em>only one</em></span> lock for the entire application instead of a separate lock for each object. Then execution of all <code class="literal">synchronized</code> blocks, regardless of their lock, would be serialized. With many threads competing for the global lock, the chance that two threads want the lock at the same time increases, resulting in more contention. So if lock requests were instead distributed over a <span class="emphasis"><em>larger</em></span> set of locks, there would be less contention. Fewer threads would be blocked waiting for locks, thus increasing scalability.</p>
<p>If a lock guards more than one <span class="emphasis"><em>independent</em></span> state variable, you may be able to improve scalability by splitting it into multiple locks that each guard different variables. This results in each lock being requested less often.</p>
<p><code class="literal">ServerStatus</code> in <a class="link" href="ch11s04.html#ch11list06" title="Example 11.6. Candidate for Lock Splitting.">Listing 11.6</a> shows a portion of the monitoring interface for a database server that maintains the set of currently logged-on users and the set of currently executing queries. As a user logs on or off or query execution begins or ends, the <code class="literal">ServerStatus</code> object is updated by calling the appropriate <code class="literal">add</code> or <code class="literal">remove</code> method. The two types of information are completely independent; <code class="literal">ServerStatus</code> could even be split into two separate classes with no loss of functionality.</p>
<p>Instead of guarding both <code class="literal">users</code> and <code class="literal">queries</code> with the <code class="literal">ServerStatus</code> lock, we can instead guard each with a separate lock, as shown in <a class="link" href="ch11s04.html#ch11list07" title="Example 11.7. ServerStatus Refactored to Use Split Locks.">Listing 11.7</a>. After splitting the lock, each new finer-grained lock will see less locking traffic than the original coarser lock would have. (Delegating to a thread-safe <code class="literal">Set</code> implementation for <code class="literal">users</code> and <code class="literal">queries</code> instead of using explicit synchronization would implicitly provide lock splitting, as each <code class="literal">Set</code> would use a different lock to guard its state.)</p>
<p>Splitting a lock into two offers the greatest possibility for improvement when the lock is experiencing moderate but not heavy contention. Splitting locks that are experiencing little contention yields little net improvement in performance or throughput, although it might increase the load threshold at which performance starts to degrade due to contention. Splitting locks experiencing moderate contention <a id="iddle2177" class="indexterm"/><a id="iddle3149" class="indexterm"/><a id="iddle4351" class="indexterm"/><a id="iddle1380" class="indexterm"/><a id="iddle1747" class="indexterm"/><a id="iddle1748" class="indexterm"/><a id="iddle2340" class="indexterm"/><a id="iddle2341" class="indexterm"/><a id="iddle2629" class="indexterm"/><a id="iddle2653" class="indexterm"/><a id="iddle2654" class="indexterm"/><a id="iddle2655" class="indexterm"/><a id="iddle2837" class="indexterm"/><a id="iddle3114" class="indexterm"/><a id="iddle3151" class="indexterm"/><a id="iddle3154" class="indexterm"/><a id="iddle3803" class="indexterm"/><a id="iddle4068" class="indexterm"/><a id="iddle4499" class="indexterm"/><a id="iddle4500" class="indexterm"/>might actually turn them into mostly uncontended locks, which is the most desirable outcome for both performance and scalability.</p>
<div class="example"><a id="ch11list06"/><p class="title"><b>Example 11.6. Candidate for Lock Splitting.</b></p><div class="example-contents">


<pre class="programlisting">@ThreadSafe
public class ServerStatus {
    @GuardedBy("this") public final Set&lt;String&gt; users;
    @GuardedBy("this") public final Set&lt;String&gt; queries;
    ...
    public synchronized void addUser(String u) { users.add(u); }
    public synchronized void addQuery(String q) { queries.add(q); }
    public synchronized void removeUser(String u) {
        users.remove(u);
    }
    public synchronized void removeQuery(String q) {
        queries.remove(q);
    }
}
</pre>
</div></div><br class="example-break"/>
<div class="example"><a id="ch11list07"/><p class="title"><b>Example 11.7. <code class="literal">ServerStatus</code> Refactored to Use Split Locks.</b></p><div class="example-contents">


<pre class="programlisting">@ThreadSafe
public class ServerStatus {
    @GuardedBy("users") public final Set&lt;String&gt; users;
    @GuardedBy("queries") public final Set&lt;String&gt; queries;
    ...
    public void addUser(String u) {
        <span class="strong"><strong>synchronized</strong></span>  (users) {
            users.add(u);
        }
    }

    public void addQuery(String q) {
        <span class="strong"><strong>synchronized</strong></span>  (queries) {
            queries.add(q);
        }
    }
    <span class="emphasis"><em>// remove methods similarly refactored to use split locks</em></span>
}
</pre>
</div></div><br class="example-break"/>
</div>
<div class="section" title="Lock Striping"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lev2sec10"/>Lock Striping</h3></div></div></div>

<p>Splitting a heavily contended lock into two is likely to result in two heavily contended locks. While this will produce a small scalability improvement by enabling two threads to execute concurrently instead of one, it still does not dramatically improve prospects for concurrency on a system with many processors. The lock splitting example in the <code class="literal">ServerStatus</code> classes does not offer any obvious opportunity for splitting the locks further.</p>
<p>Lock splitting can sometimes be extended to partition locking on a variablesized set of independent objects, in which case it is called <span class="emphasis"><em>lock striping</em></span>. For example, the implementation of <code class="literal">ConcurrentHashMap</code> uses an array of 16 locks, each of which guards 1/16 of the hash buckets; bucket <span class="emphasis"><em>N</em></span> is guarded by lock <span class="emphasis"><em>N</em></span> mod 16. Assuming the hash function provides reasonable spreading characteristics and keys are accessed uniformly, this should reduce the demand for any given lock by approximately a factor of 16. It is this technique that enables <code class="literal">ConcurrentHashMap</code> to support up to 16 concurrent writers. (The number of locks could be increased to provide even better concurrency under heavy access on high-processor-count systems, but the number of stripes should be increased beyond the default of 16 only when you have evidence that concurrent writers are generating enough contention to warrant raising the limit.)</p>
<p>One of the downsides of lock striping is that locking the collection for exclusive access is more difficult and costly than with a single lock. Usually an operation can be performed by acquiring at most one lock, but occasionally you need to lock the entire collection, as when <code class="literal">ConcurrentHashMap</code> needs to expand the map and rehash the values into a larger set of buckets. This is typically done by acquiring all of the locks in the stripe set.<sup>[<a id="ch11fn10" href="#ftn.ch11fn10" class="footnote">10</a>]</sup></p>
<p><code class="literal">StripedMap</code> in <a class="link" href="ch11s04.html#ch11list08" title="Example 11.8. Hash-based Map Using Lock Striping.">Listing 11.8</a> illustrates implementing a hash-based map using lock striping. There are <code class="literal">N_LOCKS</code> locks, each guarding a subset of the buckets. Most methods, like <code class="literal">get</code>, need acquire only a single bucket lock. Some methods may need to acquire all the locks but, as in the implementation for <code class="literal">clear</code>, may not need to acquire them all simultaneously.<sup>[<a id="ch11fn11" href="#ftn.ch11fn11" class="footnote">11</a>]</sup></p>
</div>
<div class="section" title="Avoiding Hot Fields"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lev2sec11"/>Avoiding Hot Fields</h3></div></div></div>

<p>Lock splitting and lock striping can improve scalability because they enable different threads to operate on different data (or different portions of the same data structure) without interfering with each other. A program that would benefit from lock splitting necessarily exhibits contention for a <span class="emphasis"><em>lock</em></span> more often than for the <span class="emphasis"><em>data</em></span> <a id="iddle2186" class="indexterm"/><a id="iddle1138" class="indexterm"/><a id="iddle2342" class="indexterm"/><a id="iddle2466" class="indexterm"/><a id="iddle2656" class="indexterm"/><a id="iddle2657" class="indexterm"/><a id="iddle3094" class="indexterm"/><a id="iddle3095" class="indexterm"/><a id="iddle3259" class="indexterm"/><a id="iddle3260" class="indexterm"/><a id="iddle3526" class="indexterm"/><a id="iddle3797" class="indexterm"/><a id="iddle4075" class="indexterm"/>guarded by that lock. If a lock guards two independent variables <span class="emphasis"><em>X</em></span> and <span class="emphasis"><em>Y</em></span>, and thread <span class="emphasis"><em>A</em></span> wants to access <span class="emphasis"><em>X</em></span> while <span class="emphasis"><em>B</em></span> wants to access <span class="emphasis"><em>Y</em></span> (as would be the case if one thread called <code class="literal">addUser</code> while another called <code class="literal">addQuery</code> in <code class="literal">ServerStatus</code>), then the two threads are not contending for any data, even though they are contending for a lock.</p>
<div class="example"><a id="ch11list08"/><p class="title"><b>Example 11.8. Hash-based Map Using Lock Striping.</b></p><div class="example-contents">


<pre class="programlisting">@ThreadSafe
public class StripedMap {
    <span class="emphasis"><em>// Synchronization policy: buckets[n] guarded by locks[n%N_LOCKS]</em></span>
    private static final int N_LOCKS = 16;
    private final Node[] buckets;
    private final Object[] locks;

    private static class Node { ... }

    public StripedMap(int numBuckets) {
        buckets = new Node[numBuckets];
        locks = new Object[N_LOCKS];
        for (int i = 0; i &lt; N_LOCKS; i++)
            locks[i] = new Object();
    }

    private final int hash(Object key) {
        return Math.abs(key.hashCode() % buckets.length);
    }

    public Object get(Object key) {
        int hash = hash(key);
        synchronized (locks[hash % N_LOCKS]) {
            for (Node m = buckets[hash]; m != null; m = m.next)
                if (m.key.equals(key))
                    return m.value;
        }
        return null;
    }

    public void clear() {
        for (int i = 0; i &lt; buckets.length; i++) {
            synchronized (locks[i % N_LOCKS]) {
                buckets[i] = null;
            }
        }
    }
    ...
}
</pre>
</div></div><br class="example-break"/>
<p>Lock granularity cannot be reduced when there are variables that are required for every operation. This is yet another area where raw performance and scalability are often at odds with each other; common optimizations such as caching frequently computed values can introduce “hot fields” that limit scalability.</p>
<p>If you were implementing <code class="literal">HashMap</code>, you would have a choice of how <code class="literal">size</code> computes the number of entries in the <code class="literal">Map</code>. The simplest approach is to count the number of entries every time it is called. A common optimization is to update a separate counter as entries are added or removed; this slightly increases the cost of a <code class="literal">put</code> or <code class="literal">remove</code> operation to keep the counter up-to-date, but reduces the cost of the <code class="literal">size</code> operation from <span class="emphasis"><em>O</em></span>(<span class="emphasis"><em>n</em></span>) to <span class="emphasis"><em>O</em></span>(1).</p>
<p>Keeping a separate count to speed up operations like <code class="literal">size</code> and <code class="literal">isEmpty</code> works fine for a single-threaded or fully synchronized implementation, but makes it much harder to improve the scalability of the implementation because every operation that modifies the map must now update the shared counter. Even if you use lock striping for the hash chains, synchronizing access to the counter reintroduces the scalability problems of exclusive locking. What looked like a performance optimization—caching the results of the <code class="literal">size</code> operation—has turned into a scalability liability. In this case, the counter is called a <span class="emphasis"><em>hot field</em></span> because every mutative operation needs to access it.</p>
<p><code class="literal">ConcurrentHashMap</code> avoids this problem by having <code class="literal">size</code> enumerate the stripes and add up the number of elements in each stripe, instead of maintaining a global count. To avoid enumerating every element, <code class="literal">ConcurrentHashMap</code> maintains a separate count field for each stripe, also guarded by the stripe lock.<sup>[<a id="ch11fn12" href="#ftn.ch11fn12" class="footnote">12</a>]</sup></p>
</div>
<div class="section" title="Alternatives to Exclusive Locks"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lev2sec12"/>Alternatives to Exclusive Locks</h3></div></div></div>

<p>A third technique for mitigating the effect of lock contention is to forego the use of exclusive locks in favor of a more concurrency-friendly means of managing shared state. These include using the concurrent collections, read-write locks, immutable objects and atomic variables.</p>
<p><code class="literal">ReadWriteLock</code> (see <a class="link" href="ch13.html" title="Chapter 13. Explicit Locks">Chapter 13</a>) enforces a multiple-reader, single-writer locking discipline: more than one reader can access the shared resource concurrently so long as none of them wants to modify it, but writers must acquire the lock excusively. For read-mostly data structures, <code class="literal">ReadWriteLock</code> can offer greater concurrency than exclusive locking; for read-only data structures, immutability can eliminate the need for locking entirely.</p>
<p>Atomic variables (see <a class="link" href="ch15.html" title="Chapter 15. Atomic Variables and Nonblocking Synchronization">Chapter 15</a>) offer a means of reducing the cost of updating “hot fields” such as statistics counters, sequence generators, or the reference <a id="iddle1090" class="indexterm"/><a id="iddle1091" class="indexterm"/><a id="iddle1591" class="indexterm"/><a id="iddle1713" class="indexterm"/><a id="iddle2684" class="indexterm"/><a id="iddle2869" class="indexterm"/><a id="iddle3078" class="indexterm"/><a id="iddle3079" class="indexterm"/><a id="iddle3187" class="indexterm"/><a id="iddle3251" class="indexterm"/><a id="iddle3257" class="indexterm"/><a id="iddle3258" class="indexterm"/><a id="iddle3473" class="indexterm"/><a id="iddle3474" class="indexterm"/><a id="iddle3475" class="indexterm"/><a id="iddle3692" class="indexterm"/><a id="iddle3693" class="indexterm"/><a id="iddle4064" class="indexterm"/><a id="iddle4770" class="indexterm"/><a id="iddle4943" class="indexterm"/><a id="iddle4944" class="indexterm"/><a id="iddle4949" class="indexterm"/><a id="iddle4950" class="indexterm"/><a id="iddle5111" class="indexterm"/>to the first node in a linked data structure. (We used <code class="literal">AtomicLong</code> to maintain the hit counter in the servlet examples in <a class="link" href="ch02.html" title="Chapter 2. Thread Safety">Chapter 2</a>.) The atomic variable classes provide very fine-grained (and thereforemore scalable) atomic operations on integers or object references, and are implemented using low-level concurrency primitives (such as compare-and-swap) provided by most modern processors. If your class has a small number of hot fields that do not participate in invariants with other variables, replacing them with atomic variables may improve scalability. (Changing your algorithm to have fewer hot fields might improve scalability even more—atomic variables reduce the cost of updating hot fields, but they don’t eliminate it.)</p>
</div>
<div class="section" title="Monitoring CPU Utilization"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lev2sec13"/>Monitoring CPU Utilization</h3></div></div></div>

<p>When testing for scalability, the goal is usually to keep the processors fully utilized. Tools like <code class="literal">vmstat</code> and <code class="literal">mpstat</code> on Unix systems or <code class="literal">perfmon</code> on Windows systems can tell you just how “hot” the processors are running.</p>
<p>If the CPUs are asymmetrically utilized (some CPUs are running hot but others are not) your first goal should be to find increased parallelism in your program. Asymmetric utilization indicates that most of the computation is going on in a small set of threads, and your application will not be able to take advantage of additional processors.</p>
<p>If the CPUs are not fully utilized, you need to figure out why. There are several likely causes:</p>
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><p title="Insufficent load."><b><span class="strong"><strong>Insufficent load.</strong></span> </b>It may be that the application being tested is just not subjected to enough load. You can test for this by increasing the load and measuring changes in utilization, response time, or service time. Generating enough load to saturate an application can require substantial computer power; the problem may be that the client systems, not the system being tested, are running at capacity.</p></li><li class="listitem"><p title="I/O-bound."><b><span class="strong"><strong>I/O-bound.</strong></span> </b>You can determine whether an application is disk-bound using <code class="literal">iostat</code> or <code class="literal">perfmon</code>, and whether it is bandwidth-limited by monitoring traffic levels on your network.</p></li><li class="listitem"><p title="Externally bound."><b><span class="strong"><strong>Externally bound.</strong></span> </b>If your application depends on external services such as a database or web service, the bottleneck may not be in your code. You can test for this by using a profiler or database administration tools to determine how much time is being spent waiting for answers from the external service.</p></li><li class="listitem"><p title="Lock contention."><b><span class="strong"><strong>Lock contention.</strong></span> </b>Profiling tools can tell you how much lock contention your application is experiencing and which locks are “hot”. You can often get the same information without a profiler through random sampling, triggering a few thread dumps and looking for threads contending for locks. If a thread is blocked waiting for a lock, the appropriate stack frame in the thread dump indicates “waiting to lock monitor . . . ” Locks that are mostly uncontended rarely show up in a thread dump; a heavily contended lock will almost always have at least one thread waiting to acquire it and so will frequently appear in thread dumps.</p></li></ul></div>
<p class="continued"><a id="iddle1074" class="indexterm"/><a id="iddle1075" class="indexterm"/><a id="iddle1473" class="indexterm"/><a id="iddle3355" class="indexterm"/><a id="iddle3356" class="indexterm"/><a id="iddle3358" class="indexterm"/><a id="iddle3527" class="indexterm"/><a id="iddle3607" class="indexterm"/><a id="iddle3608" class="indexterm"/><a id="iddle3610" class="indexterm"/><a id="iddle4076" class="indexterm"/><a id="iddle5113" class="indexterm"/>If your application is keeping the CPUs sufficiently hot, you can use monitoring tools to infer whether it would benefit from additional CPUs. A program with only four threads may be able to keep a 4-way system fully utilized, but is unlikely to see a performance boost if moved to an 8-way system since there would need to be waiting runnable threads to take advantage of the additional processors. (You may also be able to reconfigure the program to divide its workload over more threads, such as adjusting a thread pool size.) One of the columns reported by <code class="literal">vmstat</code> is the number of threads that are runnable but not currently running because a CPU is not available; if CPU utilization is high and there are always runnable threads waiting for a CPU, your application would probably benefit from more processors.</p>
</div>
<div class="section" title="Just Say No to Object Pooling"><div class="titlepage"><div><div><h3 class="title"><a id="ch11lev2sec14"/>Just Say No to Object Pooling</h3></div></div></div>

<p>In early JVM versions, object allocation and garbage collection were slow,<sup>[<a id="ch11fn13" href="#ftn.ch11fn13" class="footnote">13</a>]</sup> but their performance has improved substantially since then. In fact, allocation in Java is now faster than <code class="literal">malloc</code> is in C: the common code path for <code class="literal">new Object</code> in HotSpot 1.4.x and 5.0 is approximately ten machine instructions.</p>
<p>To work around “slow” object lifecycles, many developers turned to object pooling, where objects are recycled instead of being garbage collected and allocated anew when needed. Even taking into account its reduced garbage collection overhead, object pooling has been shown to be a performance loss<sup>[<a id="ch11fn14" href="#ftn.ch11fn14" class="footnote">14</a>]</sup> for all but the most expensive objects (and a serious loss for light- and medium-weight objects) in single-threaded programs (<a class="link" href="bi01.html#biblio01_009" title="Performance Myths Revisited. JavaOne presentation">Click, 2005</a>).</p>
<p>In concurrent applications, pooling fares even worse. When threads allocate new objects, very little inter-thread coordination is required, as allocators typically use thread-local allocation blocks to eliminate most synchronization on heap data structures. But if those threads instead request an object from a pool, some synchronization is necessary to coordinate access to the pool data structure, creating the possibility that a thread will block. Because blocking a thread due to lock contention is hundreds of times more expensive than an allocation, even a small amount of pool-induced contention would be a scalability bottleneck. (Even an uncontended synchronization is usually more expensive than allocating an object.) This is yet another technique intended as a performance optimization but that turned into a scalability hazard. Pooling has its uses,<sup>[<a id="ch11fn15" href="#ftn.ch11fn15" class="footnote">15</a>]</sup> but is of limited utility as a performance optimization.</p>
<div class="sidebar"><a id="ch11sb08"/><p class="title"><b/></p>
<p><a id="iddle1071" class="indexterm"/><a id="iddle1072" class="indexterm"/><a id="iddle1481" class="indexterm"/><a id="iddle2502" class="indexterm"/><a id="iddle2625" class="indexterm"/><a id="iddle3180" class="indexterm"/><a id="iddle4063" class="indexterm"/><a id="iddle4536" class="indexterm"/>Allocating objects is usually cheaper than synchronizing.</p>
</div>
</div>
<div class="footnotes"><br/><hr/><div class="footnote"><p><sup>[<a id="ftn.ch11fn07" href="#ch11fn07" class="para">7</a>] </sup>This is a corollary of <span class="emphasis"><em>Little’s law</em></span>, a result from queueing theory that says “the average number of customers in a stable system is equal to their average arrival rate multiplied by their average time in the system”. (<a class="link" href="bi01.html#biblio01_023" title="A proof of the Queueing Formula L = ?W&quot;">Little, 1961</a>)</p></div><div class="footnote"><p><sup>[<a id="ftn.ch11fn08" href="#ch11fn08" class="para">8</a>] </sup>Actually, this calculation <span class="emphasis"><em>understates</em></span> the cost of holding locks for too long because it doesn’t take into account the context switch overhead generated by increased lock contention.</p></div><div class="footnote"><p><sup>[<a id="ftn.ch11fn09" href="#ch11fn09" class="para">9</a>] </sup>If the JVM performs lock coarsening, it may undo the splitting of <code class="literal">synchronized</code> blocks anyway.</p></div><div class="footnote"><p><sup>[<a id="ftn.ch11fn10" href="#ch11fn10" class="para">10</a>] </sup>The only way to acquire an arbitrary set of intrinsic locks is via recursion.</p></div><div class="footnote"><p><sup>[<a id="ftn.ch11fn11" href="#ch11fn11" class="para">11</a>] </sup>Clearing the <code class="literal">Map</code> in this way is not atomic, so there is not necessarily a time when the <code class="literal">Striped-Map</code> is actually empty if other threads are concurrently adding elements; making the operation atomic would require acquiring all the locks at once. However, for concurrent collections that clients typically cannot lock for exclusive access, the result of methods like <code class="literal">size</code> or <code class="literal">isEmpty</code> may be out of date by the time they return anyway, so this behavior, while perhaps somewhat surprising, is usually acceptable.</p></div><div class="footnote"><p><sup>[<a id="ftn.ch11fn12" href="#ch11fn12" class="para">12</a>] </sup>If <code class="literal">size</code> is called frequently compared to mutative operations, striped data structures can optimize for this by caching the collection size in a <code class="literal">volatile</code> whenever <code class="literal">size</code> is called and invalidating the cache (setting it to -1) whenever the collection is modified. If the cached value is nonnegative on entry to <code class="literal">size</code>, it is accurate and can be returned; otherwise it is recomputed.</p></div><div class="footnote"><p><sup>[<a id="ftn.ch11fn13" href="#ch11fn13" class="para">13</a>] </sup>As was everything else—synchronization, graphics, JVM startup, reflection—predictably so in the first version of an experimental technology.</p></div><div class="footnote"><p><sup>[<a id="ftn.ch11fn14" href="#ch11fn14" class="para">14</a>] </sup>In addition to being a loss in terms of CPU cycles, object pooling has a number of other problems, among them the challenge of setting pool sizes correctly (too small, and pooling has no effect; too large, and it puts pressure on the garbage collector, retaining memory that could be used more effectively for something else); the risk that an object will not be properly reset to its newly allocated state, introducing subtle bugs; the risk that a thread will return an object to the pool but continue using it; and that it makes more work for generational garbage collectors by encouraging a pattern of old-to-young references.</p></div><div class="footnote"><p><sup>[<a id="ftn.ch11fn15" href="#ch11fn15" class="para">15</a>] </sup>In constrained environments, such as some J2ME or RTSJ targets, object pooling may still be required for effective memory management or to manage responsiveness.</p></div></div></div></body></html>

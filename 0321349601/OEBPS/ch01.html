<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><title>Chapter 1. Introduction</title><link rel="stylesheet" href="styles/convert.css" type="text/css"/></head><body><div class="chapter" title="Chapter 1. Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch01"/>Chapter 1. Introduction</h1></div></div></div>




<p><a id="iddle1387" class="indexterm"/><a id="iddle1388" class="indexterm"/><a id="iddle1457" class="indexterm"/><a id="iddle2321" class="indexterm"/><a id="iddle2349" class="indexterm"/><a id="iddle3371" class="indexterm"/><a id="iddle3372" class="indexterm"/><a id="iddle3373" class="indexterm"/><a id="iddle3674" class="indexterm"/><a id="iddle3675" class="indexterm"/><a id="iddle3941" class="indexterm"/><a id="iddle4147" class="indexterm"/><a id="iddle4231" class="indexterm"/><a id="iddle4232" class="indexterm"/><a id="iddle4280" class="indexterm"/><a id="iddle4281" class="indexterm"/><a id="iddle4323" class="indexterm"/><a id="iddle4324" class="indexterm"/>Writing correct programs is hard; writing correct concurrent programs is harder. There are simply more things that can go wrong in a concurrent program than in a sequential one. So, why do we bother with concurrency? Threads are an inescapable feature of the Java language, and they can simplify the development of complex systems by turning complicated asynchronous code into simpler straight-line code. In addition, threads are the easiest way to tap the computing power of multiprocessor systems. And, as processor counts increase, exploiting concurrency effectively will only become more important.</p>



<div class="section" title="A (Very) Brief History of Concurrency"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lev1sec1"/>A (Very) Brief History of Concurrency</h2></div></div></div>

<p>In the ancient past, computers didn’t have operating systems; they executed a single program from beginning to end, and that program had direct access to all the resources of the machine. Not only was it difficult to write programs that ran on the bare metal, but running only a single program at a time was an inefficient use of expensive and scarce computer resources.</p>
<p>Operating systems evolved to allow more than one program to run at once, running individual programs in <span class="emphasis"><em>processes</em></span>: isolated, independently executing programs to which the operating system allocates resources such as memory, file handles, and security credentials. If they needed to, processes could communicate with one another through a variety of coarse-grained communication mechanisms: sockets, signal handlers, shared memory, semaphores, and files.</p>
<p>Several motivating factors led to the development of operating systems that allowed multiple programs to execute simultaneously:</p>
<p title="Resource utilization."><b><span class="strong"><strong>Resource utilization.</strong></span> </b>Programs sometimes have to wait for external operations such as input or output, and while waiting can do no useful work. It is more efficient to use that wait time to let another program run.</p>
<p title="Fairness."><b><span class="strong"><strong>Fairness.</strong></span> </b>Multiple users and programs may have equal claims on the machine’s resources. It is preferable to let them share the computer via finer-grained time slicing than to let one program run to completion and then start another.</p>
<p title="Convenience."><b><span class="strong"><strong>Convenience.</strong></span> </b><a id="iddle1133" class="indexterm"/><a id="iddle1634" class="indexterm"/><a id="iddle1756" class="indexterm"/><a id="iddle2033" class="indexterm"/><a id="iddle2984" class="indexterm"/><a id="iddle3228" class="indexterm"/><a id="iddle3229" class="indexterm"/><a id="iddle3676" class="indexterm"/><a id="iddle3677" class="indexterm"/><a id="iddle3696" class="indexterm"/><a id="iddle3697" class="indexterm"/><a id="iddle3698" class="indexterm"/><a id="iddle4159" class="indexterm"/><a id="iddle4168" class="indexterm"/><a id="iddle4228" class="indexterm"/><a id="iddle4294" class="indexterm"/><a id="iddle4295" class="indexterm"/><a id="iddle4741" class="indexterm"/><a id="iddle4742" class="indexterm"/><a id="iddle4743" class="indexterm"/><a id="iddle4810" class="indexterm"/><a id="iddle4935" class="indexterm"/><a id="iddle4936" class="indexterm"/>It is often easier or more desirable to write several programs that each perform a single task and have them coordinate with each other as necessary than to write a single program that performs all the tasks.</p>
<p class="continued">In early timesharing systems, each process was a virtual von Neumann computer; it had a memory space storing both instructions and data, executing instructions sequentially according to the semantics of the machine language, and interacting with the outside world via the operating system through a set of I/O primitives. For each instruction executed there was a clearly defined “next instruction”, and control flowed through the program according to the rules of the instruction set. Nearly all widely used programming languages today follow this sequential programming model, where the language specification clearly defines “what comes next” after a given action is executed.</p>
<p>The sequential programming model is intuitive and natural, as it models the way humans work: do one thing at a time, in sequence—mostly. Get out of bed, put on your bathrobe, go downstairs and start the tea. As in programming languages, each of these real-world actions is an abstraction for a sequence of finer-grained actions—open the cupboard, select a flavor of tea, measure some tea into the pot, see if there’s enough water in the teakettle, if not put some more water in, set it on the stove, turn the stove on, wait for the water to boil, and so on. This last step—waiting for the water to boil—also involves a degree of <span class="emphasis"><em>asynchrony</em></span>. While the water is heating, you have a choice of what to do—just wait, or do other tasks in that time such as starting the toast (another asynchronous task) or fetching the newspaper, while remaining aware that your attention will soon be needed by the teakettle. The manufacturers of teakettles and toasters know their products are often used in an asynchronous manner, so they raise an audible signal when they complete their task. Finding the right balance of sequentiality and asynchrony is often a characteristic of efficient people—and the same is true of programs.</p>
<p>The same concerns (resource utilization, fairness, and convenience) that motivated the development of processes also motivated the development of <span class="emphasis"><em>threads</em></span>. Threads allow multiple streams of program control flow to coexist within a process. They share process-wide resources such as memory and file handles, but each thread has its own program counter, stack, and local variables. Threads also provide a natural decomposition for exploiting hardware parallelism on multiprocessor systems; multiple threads within the same program can be scheduled simultaneously on multiple CPUs.</p>
<p>Threads are sometimes called <span class="emphasis"><em>lightweight processes</em></span>, and most modern operating systems treat threads, not processes, as the basic units of scheduling. In the absence of explicit coordination, threads execute simultaneously and asynchronously with respect to one another. Since threads share the memory address space of their owning process, all threads within a process have access to the same variables and allocate objects from the same heap, which allows finer-grained data sharing than inter-process mechanisms. But without explicit synchronization to coordinate access to shared data, a thread may modify variables that another thread is in the middle of using, with unpredictable results.</p>
</div>




</div></body></html>
